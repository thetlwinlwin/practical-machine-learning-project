---
title: "Manners Prediction"
author: "ThetLwinLwin"
date: "1/8/2021"
output:
  html_document: default
  word_document: default
  keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Introduction

As the emerge of the hand wear bands or devices, it is now possible to collect large amount of data about personal activity. A group of enthusiasts took measurements about themselves regularly to improve their health and pattern of behavior. The main goal of this project to predict the manner in which they did the exercise using collected data from accelerometer.

## Exploratory Analysis

### Download and read Data

```{r}
if(!file.exists('./data/pml-training.csv') && !file.exists('./data/pml-testing.csv')){
  download.file(url='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',destfile = './data/pml-training.csv')
  download.file(url='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',destfile = './data/pml-testing.csv')
  training = read.csv('./data/pml-training.csv')
  testing = read.csv('./data/pml-testing.csv')
}else{
  training = read.csv('./data/pml-training.csv')
  testing = read.csv('./data/pml-testing.csv')
}
```

### Data Structure

The data has 160 variables and total observations of 19622. Missing values are also important in data analysis. It can be dealt with either removing or imputing the value. The detail of each variable can be read in this [link](http://groupware.les.inf.puc-rio.br/har).


```{r, warning=FALSE,message=FALSE}
require(caret)
require(rpart)
require(rattle)
Nan_value <- sapply(training, function(x) mean(is.na(x)))
#90 percent of observations is missing value.
table(Nan_value > 0.9)
```
There are 67 variables with missing values. Instead of removing rows with NA values, these variables will be removed. In fact, these 67 variables has almost 97 percentage of missing values to total ones.

## Models

The goal of this anaylsis is to predict the 'classe' object from trained model.

```{r}
table(training$classe)
```
### Data Wrangling

There are 5 class to be predicted. This is the classification problem. First,  the columns with mostly missing values are removed.

```{r, warning=FALSE}
indToRmv <- colSums(is.na(training))
filteredtraining <- training[,indToRmv==0]
filteredtesting <- testing[,indToRmv==0]
```
Next, near zero variance will be checked and removed.
```{r}
nzv <- nearZeroVar(filteredtraining)
finalTrain <- filteredtraining[,-nzv]
dim(finalTrain)
```
Near Zero Variance reduces feature from 93 to 59. 

### Data Partition 

The data are now split into **train** and **test** dataset.

```{r}
inTrain <- createDataPartition(y=finalTrain$classe, p =0.75,list=FALSE)
train <- finalTrain[inTrain,]
test <- finalTrain[-inTrain,]
set.seed(8834)
```

### Model_1

The first go-to model for classification problem is decision tree.

```{r}
decisionTree <- rpart(classe~.,data=train, method = 'class')
decisionTreePred <- predict(decisionTree,newdata = test,type = 'class')
fancyRpartPlot(decisionTree)
```
Basically, this model can predict almost correct as it solely relies on feature **X**. Variable Importance will be checked as follow.

```{r}
decisionTree$variable.importance
```
So, the features are needed to be filtered and model is rebuilt.

```{r}
train <- train[,-(1:5)]
test <- test[,-(1:5)]
decisionTree <- rpart(classe~.,data=train, method = 'class')
decisionTree$variable.importance
```
Now, there is no overwhelming features in model. The accuracy will be checked in model decision.

### Model_2

Random Forest is a flexible, easy to use machine learning algorithm that produces even without hyper-parameter tuning, a great result most of the time.

```{r}
trControl <- trainControl(method='repeatedcv', 
                          number = 5,
                          repeats = 2,
                          classProbs=TRUE)
randForest <- train(classe~.,
                    data=train, 
                    method='rf',
                    trControl=trControl,
                    importance=TRUE
                    )

randForest
```

Again, the accuracy is nearly perfect. There is a chance that this is due to overfitting the model. The clear result will be seen after confusion matrix is built in model decision.

```{r}
plot(randForest, main='effect of number of predictors on Accuracy')
```
```{r}
plot(randForest$finalModel, main = 'Model error')
```

## Model Decision

Model will be decided based on the test dataset.

```{r}
#Decision Tree
test$classe <- as.factor(test$classe)
decisionTreePred <- predict(decisionTree,newdata = test,type = 'class')
confusion1 <- confusionMatrix(decisionTreePred,test$classe)
confusion1$table

print(paste0('Accuracy is ',round(confusion1$overall['Accuracy'],3)))
```

```{r}
#Random Forest
randForestPred <- predict(randForest,newdata = test)
confusion2 <- confusionMatrix(randForestPred,test$classe)
confusion2$table

print(paste0('Accuracy is ',confusion2$overall['Accuracy']))
```

The aim of this project is to build a accurate prediction model on common incorrect gestures during barbell lifts based on several variables collected by accelerometers. For Random Forest model, the accuracy increases in validation dataset. This model perform best for this classification. Other classification models with boosting or bagging will likely be able to achieve high results.

Now, using the second model to predict the test.

```{r}
predictresult <- predict(randForest,newdata = testing)
predictresult
```
